안녕하십니까 이번에 6장 학습관련기술들 이라는 주제를 정리한 곰탱이 입니다.

갑작스런 일정 추가로 여러분들께 피해를 끼쳐 죄송하다는 말을 먼저 드리고 시작해야 될것 같습니다. 죄송합니다.

6장은 다음과 같이 구성 되어 있습니다.

----
----
----

첫번째 항목인 매개변수 갱신 입니다.
신경망 학습의 목적은 손실 함수 즉 로스 펑션의 값을 낮게 나오게 하는 매개변수의 값을 찾는 것 입니다. 이것은 매개변수의 최적 값을 찾는 일종의 최적화 문제입니다.
최적화 문제의 최적 솔루션을 찾는 방법은 상당수의 문제가 어렵고 복잡합니다.
따라서 다양한 접근 방법을 가진 솔루션들이 존재하게 됩니다.
4장에서는 그 솔루션중 하나인 SGD 방법을 이용해 가중치 매개변수의 최적값을 구했습니다.
6장에서는 sgd와 다른 최적화 기법을 소개하게 됩니다.

sgd에 대해서 간단히 복습을 하고 넘어가겠습니다.
sgd는 가중치 매개변수 w에 대한 손실함수의 기울기에 학습률을 곱한만큼의 차이를 이용해 매개변수값을 갱신합니다.

sgd는 단순하고 구현하기 쉬운 방법이지만 비효율 적인 방법입니다.
sgd의 단점을 다음의 식으로 보여드리겠습니다.
이 식의 그래프와 등고서는 다음과 같습니다
그리고 이 함수의 기울기를 그려보면 다음과 같이 표현됩니다.
이 그래프에서 보면 x 축 방향으로는 기울기의 변화가 거의 없고 Y축의 방향으로의 기울기 변화가 심합니다.

이 함수를 (-7,2)에서 시작하는 sgd를 적용해보면 다음과 같이 나옵니다
최솟값 위치인 0,0 까지 도달하는데 지그재그로 가는 경로로 비효율 적으로 탐색을 하는것을 알수 있습니다.
따라서 비효율 적인 방법을 개선할 모멘텀 ada-grad adam 방법을 소래하려고 합니다.

첫번째 방법인 모멘텀 입니다
모멘텀은 운동량을 뜻하는 단어로 물체의 움직임이 힘을 받을때의 움직임에서 착안되었습니다.
가중치 업데이트를 위해 V라는 변수가 도입되었는데 V는 다음과 같이 구성 되었습니다.

모멘텀 방법에 의한 최적화 갱신 경로는 다음과 같이 나옵니다.
sgd와 비교했을때 지그재그 경로가 덜한것을 보여줍니다.

두번째 방법인 adagrad 방법입니다.

학습을 진행할때 러닝레이트의 값이 너무 낮으면 학습 시간이 길어지고 너무 크면 학습이 제대로 되지 않습니다!
따라서 이 러닝레이트를 적절하게 정하는 방법으로 학습을 진행하면서 러닝레이트 값을 줄이는 학습률 감소(learning rate decay) 방법이 있습니다.
Adagrad는 이 학습률 감소 방법을 발전시킨 방법이며, 각각의 매개변수에 맞는 맞춤형 러닝 레이트 값을 만들어 줍니다.

Adagrad의 식은 다음과 같습니다.
h는 매개변수의 원소별로 적용되는 행렬입니다.
또한 기울기 값이 클수록 가중치 갱신량이 적게 되는게 특징 입니다.

adagrad를 이용해 최적화 갱신 경로를 그려보면 다음과 같이 나옵니다.
보시는바와 같이 최솟값을 향해 효율적으로 움직이게 됩니다.
초기에는 기울기가 크기때문에 기울기 갱신이 크게 되지 않았지만 시간이 갈수록 기울기가 줄어들어 갱신의 폭이 작아지게 됩니다.

3번째 방법인 Adam 방법입니다
adam방법은 adagrad와 모멘텀 방법을 융합한 방법입니다.
매개변수 공간을 효율적으로 탐색하고 하이퍼 파라미터의 편향 보정이 진행됩니다. 
Adam 방법을 적용해 최적화 문제를 풀어보면 다음과 같은 결과가 나옵니다. 
모멘텀 방법이 들어가 있어 공이 그릇 바닥을 구르듯 움직이고 adagrad가 있어서 왕복 횟수가 적은 모습입니다.

4가지 방법을  Mnist Dataset을 학습해 학습반복횟수와 lossfunction 값으로 나타낸 그래프 입니다
sgd 방법이 학습반복횟수 전반에 걸쳐 다른 반복들보다 높은 loss function 값을 나타내고 있고 
나머지 3개의 방법은 비슷한 성능을 보여주고 있지만 500회 안쪽의 반복횟수에서 adagrad 방법이 비교적 낮은 lossfunction 값을 보여주는 것을 나타냅니다. 

위의 Mnistdata set에서는 ada grad 방법이 최적의 방법으로 보여지지만 다른 우수한 방법이 있을수도 있습니다. 따라서 모든 문제에 항상 잘 맞는 매개변수 갱신 방법은 없으므로 여러 가지 방법을 찾고 시도해서 최적의 방법을 탐구 해야합니다.


2번장인 가중치의 초기값 입니다 
가중치의 초기값 설정은 학습의 성패를 가를 정도로 중요합니다.
가중치의 설정에 따라 기울기 소실문제 도는 표현력의 한계를 갖는 여러 문제가 발생할수도 있다.

가중치를 설정하는 방법중 하나인 가중치 감소는 학습 모델의 오버피팅을 억제해 성능을 높이는 방법입니다.
가중치의 매개변수 값을 작게 만들어 오버피팅을 방지하는것이죠.
하지만 가중치 값을 처음부터 0으로 시작하면 학습의 결과가 좋지 않게 됩니다.
가중치 값을 균일하게 세팅할경우 역전파시 모든 가중치의 값이 똑같이 갱신이 되기 때문이다.

앞의 내용을 증명할 하나의 실험을 하겠습니다.
실험조건은 레이어가 5개, 각 레이어당 뉴런이 100개, 그리고 1000개의 데이터를 정규 분포특성을 가지고 무작위로 생성한 것으로 학습을 진행 했습니다.
이 그래프는 가중치를 표준편차가 1인 정규분포로 초기화 할때의 각 레이어의 활성화 값 분포를 나타냅니다.
5개 레이어 모두 0과 1의 값이 많은것을 보여주는데 이는 시그모이드의 특성에 따른 것입니다.

시그모이드의 특성상 입력값이 특정값 이하 혹은 이상일때 출력값이 0과 1에 수렴하기 때문에 이런 양상으로 표현이 됩니다.
시그모이드에서 0과 1에 가까울수록 미분값이 작아지다가 사라지기 때문에 기울기 소실 문제가 발생합니다.

기울기 소실 문제는 역전파시 기울기가 점점 사라져서 결국에는 학습이 이루어 지지 않는 문제입니다.

가중치의 표준편차를 0.01로 정규분포로 초기화 했을 때의 각 레이어의 활성화 값 분포도 입니다.
활성화 함수의 가운데로 치우쳐져있어 뉴런의 결과가 거의 비슷한 값을 출력하는 현상을 보여줍니다.

학습이 잘되는 노드들은 왼쪽과 같은 구조를 가지는데 출력값이 서로 비슷한 노드는 오른쪽과 같은 구조를 나타내 학습이 잘 안된다는 문제를 나타냅니다.

따라서 다른 초기값을 써보겠습니다.
Xavier 초기값이라는 논문에서 권장하는 가중치인데 많은 딥러닝 프레임 워크들이 사용하는 값입니다.

특징으로는 앞층의 노드가 많아질수록 정규 분포의 값이 작아지는 특징을 가지고 있습니다.
세이비어 초기값을 사용할때의 그래프는 다음과 같이 나타내 집니다. 중간중간 모양이 찌그러 지긴 하지만 앞서 나왔던 2가지 방식 보다는 분포가 잘 되는것으로 보여집니다. 이로인해 학습이 잘 될수 있다는 것을 예측할수 있게됩니다.

relu를 사용할때의 가중치 초기값은 xavier 때와는 다릅니다.
xavier는 활성화 함수가 선형인 경우 사용하지만 relu는 he라는 앞계층에 노드의 개수가 N개라면 표준편차를 루트 n분의 2개가 되게 만드는 초기값을 가집니다.

활성화 함수로 relu를 사용할 때 가중치 초깃값에 따른 활성화 값 분포 변화 입니다. 
표준편차가 0.01일때는 각층의 활성화 값들이 아주 작아 학습이 잘 이뤄지지 않게 됩니다.
xavier 에서는 레이어가 깊어질수록 치우침이 커져서 학습할때 기울기 소실 문제를 일으키게됩니다.
3번째 그래프인 he 초기값을 사용한 것은 모든 층에 고르게 분포 되어있어 학습이 잘 될것으로 예측됩니다.

이를 실제 mnist 데이터 셋으로 학습한 그래프가 오른쪽에 있습니다. 여기서 표준편차가 0.01일때 로스펑션 값이 일정하게 나오므로 실제로도 학습이 잘 안되는 것을 보여주고
그외에 xavier he 초기값을 사용할때는 로스 펑션값이 작아져 수렴하는 것을 보여줍니다. 그중에서 he 값이 xavier 값보다 더 빠르게 로스펑션 값이 낮아지는 것을 보여줍니다.

3번재 절인 배치 정규화 입니다.
앞절에서는 각 층의 활성화 값 분포를 관찰 하면서 초기값을 설정하는 방법을 배웠습니다.
이번 절에서는 각 층이 활성화 값을 적당히 퍼트리도록 강제 하는 법을 배우겠습니다.

배치 정규화의 특징으로는 학습을 빨리 진행 가능하고 초기값에 크게 의지하지 않고 오버피팅을 억제하는 역할을 하게 됩니다. 

배치 정규화는 활성화 값이 적당히 분포 되도록 조정하는 것이르모 위의 그림과 같이 데이터 분포를 정규화 시키는 배치 정규화 계층을 새로 만든다.

미니 배치를 정규화 하는 것은 다음과 같은 식으로 표현이 가능합니다.
미니배치의 데이터를 이용해 평균을 구하고 이를 이용해 분산을 구합니다. 위의 2개를 이용해 평균 0 분산 1인 데이터로 정규화 합니다.

변환한 데이터를 확대 감마 와 이동 베타 변환을 수행합니다.
이것을 식으로 표현하면 다음과 같고 계산 그래프도 다음과 같습니다.

Mnist 데이터 셋을 사용한 결과 그래프 입니다 배치 정규화를 했을때와 안했을때를 비교하면 배치 정규화를 했을때가 큰 차이로 단 기간 내에 정확도가 높아지는 것을 보여줍니다.

4번째 절에서는 바른 학습을 위한 오버피팅 방지법을 배웁니다
기계학습에서는 종종 오버피팅이 발생합니다. 오버피팅은 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하게 됩니다 이는 정확도 저하로 이어 집니다.
오버 피팅은 매개변수가 많고 표현력이 높은 모델 훈련데이터가 적은 경우 많이 발생합니다.

다음 그림은 일부러 오버 피팅을 만들어 놓은 모델을 그래프로 나타낸 것입니다 보다시피 학습 데이터에 대한 정확도는 100%에 가깝지만 정작 테스트 데이터에 대한 정확도는 70% 정도에 머물고 있습니다. 

이러한 오버피팅을 억제하기 위한 방법중 하나로 가중치 감소 라는 방법이 있습니다 학습과정에서 큰 가중치에 대해 패널티를 부과해 오버피팅을 억제하게 됩니다.

아래 그래프는 가중치 감소를 적용했을때의 결과를 그래프로 나타낸 것입니다. 학습데이터의 정확도가 이전과 달리 테스트 데이터와의 정확도 차이가 줄었다는 것을 보여줍니다.


복잡한 모델에서는 가중치 감소만으로 대응이 힘들어진다. 따라서 드롭 아웃 이라는 기법을 추가로 사용하게 된다.
드롭 아웃 기법은 아래 그림처럼 뉴런을 임의로 삭제하면서 학습하는 방법이다.

드롭아웃을 적용하지 않은 것과 적용한 모델의 그래프 입니다. 적용 하지 않은 그래프의 학습데이터와 테스트 데이터의 차이가 적용한 것에 비해 더 큰 것을 보여줍니다 이로써 드롭아웃이 오버 피팅을 막는것에 효과가 있다는것을 보여줍니다.


5번째 절인 하이퍼 파라미터 값 찾기 입니다.
하이퍼파라미터는 각층의 뉴런수 배치 크기 매개변수 갱신 시의 학습률과 가중치 감소등을 나타냅니다
하이퍼 파라미터의 값은 매우 중요하지만 이것을 결정하기 까지는 많은 시행 착오를 격습니다. 이번절에서는 하이퍼 파라미터 값을 효율적으로 탐색하는 방법을 설명합니다.

지금까지는 학습데이터와 테스트 데이터 두가지로 분리해서 사용했지만 이 데이터 만으로는 오버피팅 여부와 범용 성능은 어디까지인지에 대한것을 평가 할 수가 없습니다. 이를 평가하기 위해 하이퍼 파라미터를 조정하기 위한 전용 데이터인 검증 데이터를 사용하게 됩니다.

하이퍼 파라미터의 최적화 과정은 다음과 같습니다.
먼저 하이퍼 파라미터 값의 범위를 설정하고 설정된 범위에서 하이퍼 파라미터의 값을 무작위로 추출합니다.
1단계에서 샘플링한 하이퍼 파리미터 값을 사용하여 학습하고 검증 데이터로 정확도를 평가합니다.
마지막으로 1단계와 2단계를 특정 횟수 동안 반복하여 그 정확도의 결과를 보고 하이퍼 파라미터의 범위를 좁힙니다.

하이퍼 파라미터 최적화를 진행하면서 검증데이터의 학습 추이를 청확도 순으로 나열한 그래프 입니다. 
베스트 1부터 베스트 5까지의 학습은 학습이 잘 되는 것을 나타냅니다 
이후 베스트 1부터 베스트 5까지의 값의 범위를 가지고 최적의 하이퍼 파라미터 값을 찾는것을 반복하게 됩니다.









