
신경망에서는 벡터와 행렬이 자주 등장합니다 벡터는 왼쪽의 그림과 같이 크기와 방향을 가진 양 으로 숫자가 일렬로 늘어선 집합으로 표현할수 있다.
이에 반해 행렬은 숫자가 2차원형태로 늘어선 것이다.
세로로 길게 늘어선 벡터를 열백터 가로로 길게 늘어선 벡터를 행벡터 라고 합니다.

아래의 코드는
배열의 사칙 연산중 더하기와 곱하기를 한 결과입니다. 행렬과 백터의 원소간 곱샘과 덧셈이 가능한 것을 보여줍니다.

브로드 캐스트는 형상이 다른 배열끼리도 연산이 가능하게 하는 기능입니다. 
다음 그림처럼 알아서 배열을 확장해서 연산을 수행합니다.

백터의 내적은 같은 형상의 백터들이 가진 요소들 끼리 곱한 것입니다.
행렬의 곱샘은 다음과 같은 그림으로 A에서는 행을 기준으로 B에서는 렬을 기준으로 곱셈을 진행합니다.

행렬의 형상 확인은 곱하는 행렬간 마주보는 요소의 원소수를 일치시키는 것입니다.
이 수가 맞지 않으면 에러를 일으키니 잘 설계를 해야합니다.

왼쪽의 그림은 신경망을 나타낸 것입니다.
신경망은 뉴런(노드) 들이 각 층간 화살표로 이어져 있는것을 나타냅니다. 맨 왼쪽을 입력층 맨 오른쪽을 출력층 가운데 층을 은닉이라고 합니다.
화살표에는 가중치가 있어서 이전 층에서 받은 신호에 곱해지게 됩니다. 
입력층을 제외한 각 층 노드에는 편향(bias) 라는 정수값이 더해지거나 빼지게 됩니다.
은닉층의 맨위의 노드에 입력되는 신호를 식으로 나타내면 다음과 같습니다.
그리고 이것을 은닉층에서 계산하는 시은 다음과 같이 나옵니다.

이러한 레이어들을 몇개 혹은 몇십개를 거쳐도 아래 그림 처럼 선형으로 밖에 표현이 되지 않는다.

이 때문에 비선형 효과를 부여하는 활성화 함수를 쓰게 됩니다 오른쪽의 시그모이드는 활성화 함수중 하나이다.
전체적으로 어떻게 돌아가는지 대략적으로 파악을 했으니 조금더 깊게 들어가서 봅시다.

신경망의 처리를 계층으로 구연해 볼건데 아파인계층을 완전연결 계층 시그모이드 계층을 시그모이드 함수에 의한 변환 으로 할때 2레이어의 계층은 다음과 같습니다.

앞에서 만든 계층들을 가지고 순서대로 진행하면 맨 뒤의 score에 추론 값이 도출이 됩니다. 이 일련의 과정을 추론 이라고 하는데 학습하지 않은 신경망은 좋은 추론을 할 수가 없다!
따라서 이번 절에서는 학습을 수행합니다.

신경망 학습에서는 학습이 얼마나 잘 되고  있는지를 알기 위한 척도로 손실(loss)를 사용합니다.
손실은 학습 데이터와 신경망이 예측한 결과를 비교하여 예측이 얼마나 나쁜지를 도출한 스칼라 값이고,
신경망에서의 손실은 손실함수(loss function)을 사용해 구한다.
다중 클래스 분류에서는 손실함수로 흔히 교차 엔트로피 오차 (cross entropy error)를 사용한다.

이를 그림으로 나타내면 다음과 같은데요 소프트 맥스에서 도출한 정답 확률을 정답 레이블을 이용해서 크로스 엔트로피 에러를 거쳐 로스값을 구합니다.

첫번째 식은 소프트 맥스 값 두번째 식은 소프트 맥스 하나에서 나온 값을 교차엔트로피 오차 수식으로 표현한것 마지막 식은 소프트 맥스 값 전체를 교차엔트로피 오차를 이용해서 계산할때의 식입니다.

이를 합친 계층으로 표현하면 다음과 같은 그림으로 표현이 가능합니다.

학습을 하기위해 미분값이 필요한데 우리는 위의 이러한 식으로 부터 각 가중치에 대한 기울기 값을 구할수 있습니다. 이를 이용해 매개변수 갱신을 진행합니다.

학습 시 신경망은 학습 데이터를 주면 손실을 출력
우리가 얻고자 하는것은 각 매개변수에 대한 손실의 기울기
이를 이용해 매개변수 갱신을 진행
이를 위해 오차 역 전파법 (back propagation)이 등장했고
오차 역 전파법을 이해하기 위해 연쇄 법칙이 등장하게 되었습니다.
두개의 함숙 ㅏ있을때 각각의 변수에 대한 미분값은 최종적으로 첫번째 변수와 결과값의 변화값으로 표현이 가능합니다.

계산그래프로 역전파를 표현한 예입니다 미분값은 체인룰을 따름을 보여줍니다.

왼쪽은 곱셈 노드의 순전파와 역전파 오른쪽은 분기 노드의 순전파와 역전파 입니다.

분기 노드가 2개로 분기한다면 Reapeat node는 N개로 분기합니다.

sum 노드는 순선파에서 NXD 행렬을 D 길이의 축으로 만듭니다 역전파는 반대로 NxD로 변환됩니다.

행렬곱의 역전파는 다음과 같이 표현이 됩니다.

시그모이드 함수를 미분하면 다음과 같은 식으로 표현이 가능하고 이것을 그림으로 나타내면 오른쪽 그림과 같이 표현이 가능합니다. 

Affine 계층을 계산 그래프로 표현한 것입니다.

매개변수를 그 기울기와 반대 방향으로 갱신하는 방법이 경사 하강법 입니다. 
갱신 기법은 이외에도 다양하지만 그중 가장 간단한 방법이 확률적경사 하강법(stochastic gradient descent) 방법이다
확률적 이라는 뜻은 무작위로 선택된 데이터(미니배치)에 대한 기울기를 이용한다는 뜻이다.
SGD는 가중치를 기울기 방향으로 일정한 거리만큼 갱신하는 방법입니다.

