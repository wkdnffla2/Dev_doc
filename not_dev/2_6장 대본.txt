6장 게이트가 추가된 RNN

RNN 복습부터 들어가겠습니다.
RNN계층은 순환 경로를 갖고 있습니다.
그 순환을 펼치면 다음 그림과 같이 표현이 가능합니다.
시계열 데이터인 x를 입력하면 h가 출력도는데 이 h는 은닉상태라고 하며 과거 정보를 저장합니다.
rnn의 특징은 이런 이전시각의 은닉상태를 이용한다는 점입니다. 이렇게 해서 과거 정보를 계승합니다.

이것의 계산그래프로 나타내면 다음과 같이 나타낼수 있습니다.
순전파는 행렬의 곱과 합, 활성화 함수 tanh에 의한 변환으로 구성됩니다.

RNN의 문제점으로 기울기 소실 또는 기울기 폭발이 있습니다.
언어 모델은 주어진 단어들을 기초로 다음에 출현할 단어를 예측 하는 일을 한다.
다음 문장을 보면 맨 끝의 단어를 맞추는 문제입니다. 앞장에도 나왔던 문제인데요 여기에서 정답은 tom입니다.
이 문제를 풀기 위해선 앞에 단어들(정보)를 기억해 둬야 합니다 따라서 이러한 정보를 RNN의 은닉 상태에 인코딩해 보관해둡니다.

이 문제를 풀기위해 RNNLM 학습의 관점에서 보면 다음과 같이 풀이가 됩니다.
그림에서는 빨간줄로 기울기가 전달이 되는데 여기에선 전달이 될수록 기울기가 작아지거나 기울기가 커져, 기울기 소실 기울기 폭발 문제가 발생하게 됩니다.

기울기 소실 기울기 폭발에 대한 원인을 좀더 자세히 보겠습니다.
그림을 보면 길이가 T인 시계열 데이터를 가정하여 T번째 정답 레이블로부터 전해지는 기울기가 전해집니다.
덧셈일때의 기울기는 그대로 전달되므로 넘어가고  tanh와 matmul에서 기울기가 변화합니다.
tanh의 미분값은 다음과 같이 그래프로 설명이 가능합니다 

미분값의 최대치는 1.0이고 x가 0으로부터 멀어질수록 작아집니다.
이말은 역전파의 기울기가 tanh을 지나칠수록 값이 계속 작아진다는 뜻입니다.
따라서 기울기 소실이 발생할수 있습니다.

행렬 곲에서의 기울기 역전파는 왼쪽의 그림과 같이 왼쪽 방향으로 갈수록 값이 곱해지게 됩니다 따라서 오른쪽 그래프 처럼 반복될수록 증가하는것을 볼수 있습니다 이게 기울기 폭발 입니다.

기울기 폭발의 대책으로는 기울기 클리핑이라는 기법을 사용합니다
g를 기울기라고 설정하고 스레시홀드값보다 커지만 2번째 줄을 실행시켜 기울기를 바꿉니다.

기울기 소실의 대책으로는 이제 배울 RNN에 게이트를 추가하는 방법이 있습니다 대표적으로 LSTM과 GRU가 있다.

LSTM에는 C라는 경로가 존재하는데 이 C를 기억셀 혹은 셀이라고 하며 LSTM의 전용 메커니즘입니다.
데이터를 자기 자신 그러니까 LSTM 계층 안에서만 주고받는 특징이 있다.

C에는 과거부터 현재 시점 T까지의 모든 정보가 저장되어 있으며 이를 이용해 tanh를 씌워 h 은닉상태를 출력하게 됩니다.
C는 이전시점의 기억c과 데이터x 은닉상태로h 부터 어떤 계산을 통해 업데이트가 됩니다.

output게이트 에서는 은닉상태 h를 출력하기전에 게이트를 씌워서 O값을 출력합니다. 
출력한 o값을 tanh를거친 셀c와 곱한 것이 식 6.2입니다.

다음으로는 forget 게이트 입니다. 
왼쪽의 그림과 같이 게이트를 이용해 기억 셀에 어떤 것을 잊을까 지시하는 게이트 입니다.

forget 게이트를 거치면서 이전의 기억셀로부터 기억이 삭제되는 기능밖에 없기 때문에 새롭게 기억을 추가하는 기능이 필요합니다. 다음 그림과 같이 g값이 기억이 추가되는 값입니다.

마지막인 input 게이트 입니다.
input gate는 g의 각 원소가 새로 추가되는 정보로써 가치가 얼만큼 가치가 있는지를 판단합니다.
g값과 i값을 곱해서 기억셀에 추가하면 완성입니다.

이러한 것들이 어떻게 기울기 소실을 없애주는지 설명하겠습니다.
C에서의 역전파를 주목하면 되는데요 기억셀에서의 역전파는 다음의 그림과 같습니다.
+연산은 기울기를 그대로 보내므로 넘어가고 곱 연산만 보겠습니다.
곱연산은 행렬곱이 아닌 원소별 곱생이므로 매 시각T 마다 다른 게이트를 이용해 원소별 곱을 계산하기 때문에 곱셈의 효과가 누적되지 않아 기울기 소실이 거의 일어나지 않게 됩니다.
또한 forget 게이트가 잊어야 한다는 기억셀의 값의 기울기는 약화되어 전달되고 반대의 경우 기울기는 그대로 전달이 되게 됩니다.

드디어 LSTM 구현의 장입니다.
아래의 식들은 LSTM 한단계를 정리한 수식입니다.
식 6.6에서의 공통적으로 적용된 부분이 있는데 이를 아핀 변환이라고 합니다.

이것을 정리하면 다음과 같이 표현이 가능합니다.
이렇게 모아 놓으면 1회의 계산으로 끝낼수 있기 때문에 계산 시간이 빨라지게 됩니다.
또한 가중치 값이 한곳에 모여있기 때문에 소스코드 작성도 간편해 집니다.

이를 계산 그래프로 나타내면 다음과 같이 표현이 가능해 집니다.
여기서의 slice는 아핀 결과에서의 값을 균등하게 4조각으로 나눠 꺼내주는 역할을 합니다.

slice 의 역전파는 나누어 주었던 행렬들을 하나로 합치게 됩니다.

time lstm 구현은 T개분의 시계열 데이터를 한꺼번에 처리하는 계층입니다.
아래의 그림과 같이 T개의 lstm 계층으로 구성이 됩니다.

앞서 배운 RNN에서는 Truncated bptt를 수행 했습니다 역전파의 연결을 임의의 갯수로 끊었었지만 순전파의 흐름은 끊지 않았기 때문에 아래의 그림처럼 은닉상태와 기억 셀을 인스턴스 변수로 유지하여 다음 시각에서 사용할수 있게 했습ㄴ다.

time lstm까지의 구현이 끝났으면 본래의 주제인 언어 모델을 구현할 차례입니다 여기서 구현할 언어 모델은 앞장에서 구현한 언어 모델과 거의 같습니다. 앞장에서는 time RNN 계층이 차지하던 부분이 Time LSTM계층으로 바뀌었습니다.

구현을 하고 학습을 진행한 결과입니다 250번 넘게 반복을 하니 퍼플렉시티가 100 정도에 머무는 것을 보여줍니다.
참고로 최신 연구에서는 퍼플렉시티가 60 밑을 나타내고 있습니다.

RNNLM의 개선점을 3가지 정도 설명하고 얼마나 좋아졌는지를 평가해보겠습니다.
RNNLM으로 보다 정확한 모델을 만들고자 한다면 다음 그림과 같이 LSTM계층을 여러개 쌓아 효과를 볼수가 있습니다.
이렇게 계층을 쌓게 되면 더 복잡한 패턴을 학습할수 있게 됩니다.
쌓는 층의 최적화된 수는 정해진게 없으니 경험적으로 하이퍼 파라미터를 설정해서 테스트를 진행해야합니다.

앞에서 계층을 쌓으면 복잡한 패턴을 학습할수 있다고 했습니다.
하지만 이럴 경우 오버피팅 문제가 발생할수 있고 또 RNN은 일반적인 신경망 구조보다 쉽게 과적합을 일으킬수가 있습ㄴ디ㅏ.
따라서 과적합을 억제하는 방법중에는 훈련 데이터의 양 늘이기와 모델의 복잡도를 줄이는 방법이 있습니다.
그중의 한가지인 드롭아웃 방법은 계층 내의 뉴련 몇개를 무작위로 무시하고 학습하는 방법입ㄴ디ㅏ.

드롭아웃 계층의 적절한 위치는 lstm층의 위아래 쪽 입니다 lstm 양옆에 위치할 경우 연속된 시간의 흐름에 따른 정보가 소실이 될 가능성이 있습니다.

하지만 다음 그림처럼 같은 계층의 드롭아웃 끼리 같은 마스크를 이용해서 시간방향 드롭아웃도 이용하는 케이스가 있습니다.
여기서 마스크란 데이터의 통과 차단을 결정하는 이진형태의 패턴입니다.
같은 마스크를 공유함으로써 마스크가 고정되어 정보를 잃는 방법도 고정되므로 일반적인 드롭아웃과는 달리 정보가 지수적으로 손실되는 사태를 피할수 있습니다.

언어 모델을 개선하는 아주 간단한 방법중 하나로 가중치 공유 라는 방법이 있습니다.
아래 그림처럼 임베딩 계층의 가중치와 아핀 계층의 가중치를 연결하는 기법이 가중치 공유입니다.
두 계층이 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄고 정확도도 향상되는 방법입니다!
실제로 위의 3가지 개선 법을 적용하면 모델의 퍼플렉서티가 100 이하로 나오는 것을 보여줍니다.

이번장에서는 게이트가 추가된 RNN을 공부했습니다 다음장에서는 언어 모델을 사용해 문장을 생성해볼겁니다 그리고 기계 번역처럼 한 언어를 다른 언어로 변환하는 모델을 공부할 것입니다.
그리고 꼭꼭 코드 실습 해보세요 이해하는데 도움이 많이 됩니다. 보통 LSTM 공부하고 나면 많은 분들이 주식 데이터로 학습해보시더라구요 ㅎㅎ;;






