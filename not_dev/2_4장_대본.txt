안녕하세요 4장의 word2vec 의 속도개선 주제로 발표를 맡은 곰탱이 입니다.
이번장의 목표는 word2vec의 속도를 개선시키는것을 목표로 합니다.
word2vec에서 Embedding이라는 계층을 도입하고 네거티브 샘플링이라는 새로운 손실 함수를 도입합니다.

목차는 다음과 같이 구성이 되어있습니다.
-
-
-
-

앞장에서의 word2vec는 다음 그림과 같이 구현되었습니다.
입력층에서는 단어 2개를 맥락으로 사용해 맥락 사이에 들어갈 단어(타깃)을 추측을 합니다.
이때 입력측 가중치와의 행렬 곱으로 은닉층이 계산되고 출력측 가중치와의 행렬 곱으로 각 단어의 점수를 구하게 됩니다.
이 점수에 소프트맥스 함수를 적용해 각 단어의 출현 확률을 얻고 이 확률을 정답 레이블과 비교하혀 손실값을 구하게 됩니다.
이러한 cbow모델은 말뭉치가 작을때는 연산에 아무런 문제가 없습니다.

그러면 어휘가 100만개일때는 어떨까요?
어휘가 100만개가 되면 뉴런의 갯수가 100만개가 됩니다. 계산의 크기도 물론 증가하게 되어 많은 시간이 소요될것은 자명합니다.
정확히는 입력층과 은닉층과의 계산과 은닉층과 출력층 간의 계산 출력층을 softmax 처리하는 계산에 시간이 많이 소모될 것입니다.
따라서 이 문제점 들에 대한 해결방법을 제시하고자 합니다.

첫번째 문제인 입력층과 은닉층과의 계산 크기를 줄이는 방법입니다.
Embedding 계층을 추가하는 것인데요. word2vec 에서는 단어를 원핫 표현으로 바꿔서 행렬간 곱을 계산했습니다.
어휘의 갯수가 100만개의 원핫코드와 은닉층의 곱샘 결과는 다음과 같습니다.
원핫코드로 표현을 하기 때문에 정보를 가진 열을 제외한 나머지 부분은 0으로써 계산할때 불필요한 부분이 됩니다.
따라서 이것을 행렬곱셈으로 연산을 하기보다 필요한 부분의 가중치를 빼오는 방법으로 대체가 가능합니다.
이것이 embedding 계층입니다.

임베딩 계층의 순전파와 역전파 구조는 다음의 그림과 같습니다.

책에서 구현하는 코드를 기준으로 역전파를 진행하게 되면 idx의 원소가 중복 될때 문제가 발생합니다.
바로 할당하는 것이기 때문에 하나의 기울기밖에 dW에 할당이 되지 않게 됩니다 이를 막기 위해 덧셈을 해서 중복된 원소의 기울기의 소실을 방지합니다.

남은 문제점은 은닉층 이후의 계산과 soft max 계산입니다.
이문제는 네거티브 샘플링을 이용해 해결이 가능합니다. 또한 네거티브 샘플링을 이용하면 어휘가 아무리 많아져도 계산량을 낮은 수준에서 유지가 가능합니다.

앞에서와 마찬가지로 어휘가 100만개 은닉층의 뉴런이 100개일때의 word2vec를 예로들어 설명하겠습니다.
은닉층과 출력층의 계산의 크기는 100*100만 을 가집니다. 시간도 오래걸리고 메모리도 많이 사용을 하게 됩니다.
또한 soft max의 계산횟수도 어휘의 수에 비례해 증가하게 됩니다.

이러한 문제를 해결하기 위해 네거티브 샘플링 기법을 도입하게 됩니다.
네거티브 샘플링의 핵심 아이디어는 다중 분류를 이중분류로 변환 하는것입니다.
지금까지 우리는 word2vec로 100만개의 단저 중에서 타겟 단어(1개)를 추측하는 다중 분류로 문제를 다뤘습니다.
이런 문제를 이진 문제 다시말해 참, 거짓으로 분류하는 방법으로 바꾼 것입니다.
이렇게 하면 출력층에는 뉴런을하나만 준비하면 됩니다.
이때의 활성화 함수는 확률을 나타내는 소프트 맥스 함수가 아닌 시그모이드 함수를 사용합니다.

시그모이드의 계산 흐름과 그래프는 다음과 같습니다.
거짓에 가까울수록 0에 가까운 값이 참에 가까울수록 1에 가까운 값이 나오게 됩니다.

시그모이드에서 나온 값을 가지고 크로스엔트로피 에러를 이용해 로스값을 구합니다 이 로스값이 작은게 정답과 유사한 것입니다.

앞서 모든 문제의 해결방법을 적용한 CBOW의 모델의 전체 그림은 다음과 같습니다.

지금 까지는 시그모이드 함수를 이용해 결과가 긍정적인 것에 대해서만 학습을 진행했습니다.
하지만 정답이 아닌것을 입력하면 어떤것이 나오는지 학습을 진행하지 않았고 시그모이드 함수를 이용하면 거짓을 나타내는 결과값이 0인것만 배웠습니다.
이제는 어떻게 해야 0이 나오게 되는지 알아봅니다.

앞서 말했듯이 긍정적인 예는 1에 가까운값을 부정적인 예는 0에 가까운 값을 나타낸다고 배웠습니다.
그럼 모든 부정적인 예를 학습하면 되지 않을까요?
정답은 아닙니다. 모든 부정적인 예를 학습한다면 앞에서 계산량을 줄인 것이 소용 없게 됩니다.
따라서 몇몇개의 단어만 샘플링해 학습을 진행하게 됩니다.

네거티브 샘플링을 진행할때 무작위로 샘플링을 진행하는 방법보다 말뭉치의 통계 데이터를 기초로 샘플링하는 방법이 효과적입니다.
많이 등장하는 단어를 많이 추출하고 적게등장하는 단어를 적게 추출하는 것이죠
단어빈도를 추출해 확률분포로 나타내서 이것을 기반으로 추출을 진행합니다.
하지만 단어의 빈도가 아주 작다고 샘플링이 안되는 경우가 생겨 학습이 제대로 안될 경우가 발생하기 때문에 확률분포에 각 요소를 0.75제곱하는 방법으로 약간의 수정을 진행합니다.

word2vec를 사용한 어플리케이션의 예로 분산 표현을 이용해 비슷한 단어를 찾을수가 있습니다.
자연어 처리 분야에서 단어의 분산표현이 중요한 이유는 전이학습이 가능하다는 점인데요
전이 학습은 한 분야에서 배운 지식을 다른 분야에도 적용하는 방법입니다.
자연어 문제를 풀때 먼저 큰 말뭉치로 학습을 끝난 후 분산표현을 각자의 작업에 이용하는 방식으로 진행합니다.

word2vec를 이용해서 단어의 분산 표현을 얻었을때 이 분산 표현을 평가 해야 합니다.
하지만 단어의 최종 정확도를 평가하려면 단어의 분산 표현을 이용하여 다른 머신 러닝 시스템을 학습해야 평가가 이루어지기 때문에 시간이 오래걸리게 되죠
따라서 단어의 유사성이나 유추 문제를 활용해 평가를 진행합니다.
유추 문제를 이용하면 단어의 의미나 문법적인 문제를 제대로 이해하고 있는지를 대략적으로 측정이 가능하게 됩니다

이상으로 4장에 대한 발표를 마치겠습니다.


