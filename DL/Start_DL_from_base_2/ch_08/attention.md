# 8. 어텐션

- 앞장에서는 RNN을 사용해 문장을 생성함.

- 2개의 RNN을 연결하여 하나의 시꼐열 데이터를 다른 시계열 데이터로 변환도 했다

- 이를 seq2seq 라고 하며 덧셈 문제를 푸는데 성공했다.

- 이번 장에서는 seq2seq2의 가능성 그리고 RNN의 가능성을 더 깊히 탐험한.

## 8.1 어텐선의 구조

- seq2seq를 강력하게하는 어텐션 메커니즘이라는 아이디어를 소개한다.

- 어텐션 메커니즘 덕분에 seq2seqsms 필요한 정보에만 주목이 가능하다.

- 이번절에서는 현재의 seq2seq가 가지고있는 문제를 살펴보고 어텐션의 구조를 설명하면서 구현까지 해본다.


### 8.1.1 seq2seq의 문제점

- seq2seq에서는 encoder가 시계열 데이터를 인코딩 한다 그리고 인코딩된 정보를 디코더로 전달한다.

- 이때 인코더의 출력은 고정 길이의 벡터이다. 하지만 이 고정 길이는 문제가 있다.

- 입력문장의 길이가 길어도 항상 같은 길이의 벡터로 변환된다는 뜻이다.

![그림 8-1](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%208-1.png)

- 현재의 인코더는 아무리 긴 문장이라도 고정 길이의 벡터로 변환한다.

- 그러므로 필요한 정보가 벡터에 다 담기지 못하게 된다.


### 8.1.2 Encoder 개선 

- 우리는 지금까지 LSTM 계층의 마지막 은닉 상태만을 Decoder 에게 전달했다.

- 그러나 Encoder 출력의 길이는 입력 문장의 길이에 따라 바꿔주는게 좋다.

- 이 점이 인코더의 개선 포인트 이다.

- 그림 8-2 처럼 시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용 하는 것이다.

![그림 8-2](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%208-2.png)

- 그림 8-2 처럼 각 시각 (각 단어) 의 은닉 상태 벡터를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻을 수 있다.

- 그림 8-2 에서 5개의 단어가 입력 되었고 이때 엔코더는 5개의 벡터를 출력한다. 이것으로 인코더는 하나의 고정 길이 벡터 라는 제약으로부터 해방된다.

- 그림 8-2 에서 주목할 것은 LSTM 계층의 은닉 상태의 내용이다.

- 각 시각의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다는 것이다.

- 예를 들어 고양이 라는 단어를 입력했을 때의 LSTM 계층의 출력은 직전에 입력한 고양이라는 단어의 영향을 가장 크게 받는다.

- 이 은닉 상태 벡터는 고양이의 성분이 많이 들어간 벡터이다.

- 따라서 인코더가 출력하는 hs 행렬은 각 단어에 해당하는 벡터들의 집합이라볼수 있다.

![그림 8-3](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%208-3.png)

### 8.1.3 Decoder 개선

- 인코더는 각 단어에 대응하는 LSTM 계층의 은닉 상태 벡터를 hs로 모아 출력한다.

- 그리고 이 hs 가 decoder에 전달되어 시계열 변환이 이루어 진다.

![그림 8-4](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%208-4.png)

- 인코더의 LSTM 계층의 마지막 은닉 상태를 디코더의 LSTM 계층의 첫 은닉 상태로 설정한 것이다. 이 디코더의 계층 구성은 그림 8-5로 표현이 가능하다.

![그림 8-5](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%208-5.png)

- 위 그림에서는 hs 에서 마지막 줄만 빼내어 디코더에 전달한다 그래서 이 hs 를 전부 사용하도록 디코더를 개선해본다.

- 사람이 문장을 번역할때는 단어에 주목하여 그 단어의 변환을 수시로 진행한다.

- 이와 같은 과정을 seq2seq로 재현해본다.

- 도착어 단어와 대응관계에 있는 출발어 단어의 정보를 골라내는 것이 목표이다.

- 그리고 그 정보를 이용하여 번역을 수행한다.

- 필요한 정보에만 주목하여 그 정보로 부터 시계열 변환을 수행하는 것이 목표이다.

- 이 구조를 어텐션 이라 한다.

- 우리가 구현하고자 하는 신경망의 계층 구성은 그림 8-6과 같다.

![그림 8-6](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%208-6.png)