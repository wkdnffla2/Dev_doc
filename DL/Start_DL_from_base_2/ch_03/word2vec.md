# 3. word2vec

- 앞장에 이어 이번장의 주제도 단어의 분산 표현이다.
- 앞 장에서는 통계 기반 기법으로 단어의 분산 표현을 얻었다면 이번 장에선 추론 기반 기법을 살표본다..

- 이번 장의 목표는 단순한 word2vec 구현하기 이다.

- 처리 효율을 희생하는대신 이해하는데 쉽도록 구성한 것이다.

- 따라서 큰 데이터 셋은 모르겠지만 작은거는 처리가 가능하낟.

## 3.1 추론 기반 기법과 신경망

- 단어를 벤터로 표현하는 방법은 크게 통계 기반 기법과 추론 기반 기법이다.

- 이번 절에서는 통계 기반 기법의 문제를 지적하고 그 대안인 추론 기반의 이점을 거시적 관점에서 설명한다.

### 3.1.1 통계 기반 기법의 문제점

- 통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현
- 이 방식은 대규모 말뭉치를 다룰때 문제가 발생

- 거대 행렬에서 SVD는 현실적이지 않다.

- 통계 기반 기법은 말뭉치 전체의 통계를 이용해 단 1회의 처리 만에 단어의 분산 표현을 얻는다.
- 추론 기반 기법에서는 신경망을 이용하는 경우 미니배치로 학습하는것이 일반적이다.

- 그림 3-1은 이 두 기법의 큰 차이를 보여줍니다.
![그림 3-1](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-1.png);

- 미니 배치 학습은 말뭉치의 어휘 소가 많아 svd 등 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망을 학습시킬수 있다.

### 3.1.2 추론 기반 기법 개요

- 추론이란 그림 3-2 처럼 주변 단어가 주어졌을때 ? 안에 어떤 단어가 들어가는지를 추측하는 작업이다.

![그림 3-2](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-2.png);

- 위 그림 처럼 추론 문제를 풀고 학습하는 것이 추론 기반 기법이 다루는 문제이다.
- 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습시키는 것이다.

- 모델 관점에서 보면 이 추론 문제는 그림 3-3 처럼 보인다.

![그림 3-3](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-3.png);

- 추론 기반 기법에는 어떠한 모델이 등장한다.

- 우리는 이 모델로 신경망을 사용한다.

- 모델은 맥락 정보를 입력받아 출현할 가능성이 있는 각 단어의 출현 확률을 출력한다.

- 이러한 틀 안에서 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습시킨다.

### 3.1.3 신경망에서의 단어 처리

- 신경망에서는 you 와 say 등의 단어를 있는 그대로 처리할수 없으니 단어를 고정 길이의 벡터로 변환 해야한다

- 이때 사용하는 것이 원핫 벡터 이다.

- you say goodbye and i say hello . 라는 한 문장을 예로 들어본다

- 이 말뭉치에는 어휘가 총 7개가 등장한다 이중 두 단어의 원핫 표현을 그림 3-4 에 표현했다.

![그림 3-4](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-4.png);

- 그림 3-4 처럼 단어는 텍스트 단어 ID 그리고 원핫 표현 형태로 표현이 가능하다.

- 어휘 수만큼의 원소를 갖는 벡터를 준비하고 인덱스가 단어 ID와 같은 원소를 1로 나머지는 모두 0으로 설정한다.

![그림 3-5](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-5.png);

- 그림 3-5 처럼 입력층의 뉴런은 총 7개이다

- 단어를 벡터로 나타낼 수 있고, 신경망을 구성하는 계층들은 벡터를 처리할 수 있다.

- 다시말해 단어를 신경망으로 처리할 수 있다는 뜻이다.

- 그림 3-6 은 원핫 표현으로 된 단어 하나를 완전 연결 계층을 통해 변환하는 모습을 보여준다.

![그림 3-6](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-6.png);

- 그림 3-6 에서는 뉴런 사이의 결합을 화살표로 그렸다.

- 가중치를 명확하게 보여주기 위해 그림 3-7로 표현한다.

![그림 3-7](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-7.png);


- 원핫 벡터와 가중치를 곱한 부분은 그림 3-8 과 같다.

![그림 3-8](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-8.png);

- 가중치로부터 행 벡터를 뽑아낼 뿐인데 행렬 곱을 계산하는 건 비효율적이라고 생각하기 때문에 이 점은 4.1 에서 개선할 예정이다.

## 3.2 단순한 word2vec

- 앞 절에서는 추론 기반 기법을 배우고 신경망으로 단어를 처리하는 방법을 코드로 살펴봤다.

- 이것으로 준비는 끝나고 word2vec를 구현할 차레이다.

- 그림 3-3의 모델을 신경망으로 구축하는 것이다.

- 이번에 사용할 신경망은 word2vec 에서 제안하는 CBOW 모델이다.

### 3.2.1 CBOW 모델의 추론 처리

- CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다.

- 타깃은 중앙 단어이고 그 주변 단어들이 맥락이다.

- 우리는 이 CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낼 것이다.

- CBOW 모델의 입력은 맥락이다. 맥락은 단어들의 목록이다.

- 이상을 기초로 CBOW 모델의 신경망을 그림 3-9처럼 그릴수 있다.

![그림 3-9](../DLFromScratch2-master/equations_and_figures_2/deep_learning_2_images/fig%203-9.png);

- 입력층이 2개있고 은닉층을 거쳐 출력층에 도달한다.

- 두 입력층에서ㅗ 은닉층으로의 변환은 똑같은 완전 연결 계층이 처리한다.

- 은닉층에서 출력층 뉴런으로 변환은 다른 완전연결계층이 처리한다.

- 입력층이 여러개이면 전체를 평균 처리 한다.

- 출력층의 뉴런은 총 7개이다. 뉴런 하나하나가 각각의 단어에 대응한다는 점이다.

- 출력층 뉴런은 각 단어의 점수를 뜻하며 값이 높을수록 단어의 출현 확률도 올라간다.

